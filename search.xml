<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo NexT主题中集成gitalk评论系统]]></title>
    <url>%2F2018%2F01%2F20%2Fgitalk%2F</url>
    <content type="text"><![CDATA[记录在NexT主题中添加gitalk评论系统的步骤。gitalk：一个基于 Github Issue 和 Preact 开发的评论插件详情Demo可见：https://gitalk.github.io/ Register Application在GitHub上注册新应用，链接：https://github.com/settings/applications/new参数说明：Application name： # 应用名称，随意Homepage URL： # 网站URL，如https://asdfv1929.github.ioApplication description # 描述，随意Authorization callback URL：# 网站URL，https://asdfv1929.github.io 点击注册后，页面跳转如下，其中Client ID和Client Secret在后面的配置中需要用到，到时复制粘贴即可： gitalk.swig新建/layout/_third-party/comments/gitalk.swig文件，并添加内容：1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/gitalk/dist/gitalk.css&quot;&gt; &lt;script src=&quot;https://unpkg.com/gitalk/dist/gitalk.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; var gitalk = new Gitalk(&#123; clientID: &apos;&#123;&#123; theme.gitalk.ClientID &#125;&#125;&apos;, clientSecret: &apos;&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;&apos;, repo: &apos;&#123;&#123; theme.gitalk.repo &#125;&#125;&apos;, owner: &apos;&#123;&#123; theme.gitalk.githubID &#125;&#125;&apos;, admin: [&apos;&#123;&#123; theme.gitalk.adminUser &#125;&#125;&apos;], id: location.pathname, distractionFreeMode: &apos;&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;&apos; &#125;) gitalk.render(&apos;gitalk-container&apos;) &lt;/script&gt;&#123;% endif %&#125; comments.swig修改/layout/_partials/comments.swig，添加内容如下，与前面的elseif同一级别上：12&#123;% elseif theme.gitalk.enable %&#125; &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt; index.swig修改layout/_third-party/comments/index.swig，在最后一行添加内容：1&#123;% include &apos;gitalk.swig&apos; %&#125; gitalk.styl新建/source/css/_common/components/third-party/gitalk.styl文件，添加内容：1234.gt-header a, .gt-comments a, .gt-popup a border-bottom: none;.gt-container .gt-popup .gt-action.is--active:before top: 0.7em; third-party.styl修改/source/css/_common/components/third-party/third-party.styl，在最后一行上添加内容，引入样式：1@import &quot;gitalk&quot;; _config.yml在主题配置文件next/_config.yml中添加如下内容：12345678gitalk: enable: true githubID: github帐号 # 例：asdfv1929 repo: 仓库名称 # 例：asdfv1929.github.io ClientID: Client ID ClientSecret: Client Secret adminUser: github帐号 #指定可初始化评论账户 distractionFreeMode: true 以上就是NexT中添加gitalk评论的配置，博客上传到GitHub上后，打开页面进入某一博客内容下，就可看到评论处。]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>Gitalk</tag>
        <tag>NexT</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n学习记录 Part1 Image Classification]]></title>
    <url>%2F2018%2F01%2F14%2Fcs231n-part-1-image-classification%2F</url>
    <content type="text"><![CDATA[此为学习斯坦福cs231n课程所做笔记。 Image ClassificationMotivation本节主要讲了图像分类问题，它是一个从固定的类别集合里分配一个标签（label）给输入图像的任务。这是计算机视觉的核心问题，虽然看似简单，但是有很多的实际应用。其他许多看似不同的计算机视觉任务（比如说：目标检测，图像分割）都可以归类为图像分类。 Exmaple举个栗子，上图中，一个图像分类模型获取了一个图像，并赋给它4个标签值（cat、dog、hat、mug）。如图所示，对于计算机来说，一个图像可被视为是一个大型的3维数组。在这里，猫图有248像素宽，400像素高，且有三个颜色通道：Red、Green、Blue（即RGB）。因此，该图像是由248x400x3个数字组成，也就是总共297600个数字。其中每个数字都是一个在0（black）到255（white）范围内的整数值。我们的任务就是将这组几十万的数字转换成单个的标签label，比如说“cat”。 注：图像分类的任务是为一个给定图像预测一个标签值（或是关于标签的一个分布，比如上图中四种标签的概率分布）。图像是一个由0~255范围内数字组成的3维数组，大小为 Width x Height x 3，其中3表示为三种颜色通道：Red、Green、Blue。 Challenges因为识别一个视觉上的目标（比如说，猫）对于人类来说是相对微不足道的，但是从计算机视觉算法的角度上去考虑涉及到的困难挑战是十分值得的。下面列出部分挑战： Viewpoint variation（视角变化）：一个目标实体对于照相机来说可以多种方式进行定向。 Scale variation（规模变化）：视觉目标通常会存在多种大小变化（真实世界中的大小，不仅仅是图像中的大小程度） Deformation（形变）：许多目标实体不是那种刚体，也可能会以极端方式发生变形。 Occlusion（遮挡）：目标实体可能会被遮挡，有时候目标只有一小部分（只有少数像素）可见。 Illumination conditions（光照条件）：光照条件在像素级上影响较大。 Background clutter（背景混乱）：目标对象可能混入其环境之中，使得他们难以被识别出来。 Intra-class variation（类内变化）：目标对象的类别可能相对广泛，比如说椅子。这些目标有许多不同的类型，每个类型都有自己的外观。 一个好的图像分类模型必须是对这些变化的交叉组合保持不变性，同时对类间变化保持敏感性。 Data-driven approach提供给计算机每类许多样本，之后研发出一个学习算法去训练这些样本，学习每一类的视觉外观。这种方法被称为数据驱动方法，因为它依赖于首先收集标注过的图像的训练数据集。如下图所示： Image classification pipeline图像分类的任务是输入代表图像的像素数组，输出一个分配给它的标签值。 完整的流程大致如下： 输入：输入N个图像，其中每个图像都被标注为K个不同类别中的一个，称之为训练集 学习：任务是学习训练集，学习每一类外观是什么样子的。称这个步骤为训练一个分类器，或学习一个模型 评估：最后，通过预测新的图像集合的标签来评估这个分类器的性能 Nearest Neighbor Classifier最近邻分类器，与卷积神经网络无关，但却非常的实用，它给出了一个图像分类问题基本解决方法的概念。 CIFAR-10图像分类数据集CIFAR-10包含了60000张32x32像素的小图片，每个图片都被标注为10个类别中的一个。这60000张图片被分为含有50000张图片的训练集和含有10000张图片的测试集。 下图中可以看到10类中每个各10张随机的图像样本：上图左侧是CIFAR-10数据集的样例图像；右侧中第一列是几张图像，之后的列中是根据像素级距离来判断的与第一列中图像最近的10张图像。 最近邻分类器接收一个测试图像，将之与训练集中的每个图像进行比较，之后输出最近训练图像的标签值，即为分类器的预测值。在上图的右侧中我们可以看到根据测试图像生成的距离最近的10张训练图像，其中10个测试图像只有少数几个检索到同一类的目标图像，其余的都不是该类图像。比如说，在第8行中，与测试图像（马）距离最近的训练图像中却是一辆车，这可能是因为两幅图中大范围的黑色背景所导致。因此，该马类图像在这种情况下被错误地标注为是一辆车。 L1 Distance接下来介绍下如何对两幅图像进行比较，也就是两个32x32x3的块之间的比较。一个最简单的方法是逐个比较图像的像素，并对其中所有的差值求和。换句话说，给定两张图像，分别表示为$I_1$、$I_2$，比较它们的一个合理方法便是L1距离：其中求和操作是面向所有像素的。 过程可视化：上图是一个在像素级上使用L1距离对两张图像进行比较的简单例子（例子中只是列出一个通道）。两幅图像各元素对应值相减，然后将所有的差值相加，得到单个数值。如果两张图像相同，则这单个数值将会是0。如果两张图像差异较大，则该值相对较大。 Code Implement首先载入数据为4个数组：训练数据/标签、测试数据/标签。Xtr（size: 50000x32x32x3）包含了训练集中的所有图像，对应的一维数组Ytr（length: 50000）包含了训练标签（0~9）：1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/')# 扁平化处理Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows 10000 x 3072 之后便是训练与评估：12345nn = NearestNeighbor() # 创建一个最近邻分类器的实例nn.train(Xtr_rows, Ytr) # 在训练集和标签上训练分类器Yte_predict = nn.predict(Xte_rows) # 在测试图像上预测标签值# 打印输出分类准确度print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) 注：1）一般都是用准确度来作为评估标准，预测正确的数量占比。2）所有分类器都有一个公共的接口：train(X, y)方法，表示从数据和标签中学习模型。3）predict(X)方法，接收新的数据，并预测其标签值。 下面是一个使用L1距离的简单最近邻分类器的代码实现：123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred The choice of distance向量之间有多重方式去计算它们的距离。另一种常见的选择便是L2距离，其具有计算两个向量之间欧氏距离的几何解释。距离公式如下：也是先计算像素级别上的差值，然后对其值求平方，之后将所有的值相加，最后对值开根号。代码方面，只要对之前的代码做些许修改即可，将计算距离的那行代码替换为如下一行：1distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1)) 上面代码中使用了np.sqrt，但在实际最近邻应用中，我们省去了开根号这一操作，因为开根是一个单调函数。也就是说，它控制了距离的绝对大小的量级，但仍保留其排序，所以有或没有开根，最近邻的比较没有影响。 L1 vs. L2L1/L2距离是p范数中最常用的特例。 k-Nearest Neighbor Classifierk近邻分类器：找出与测试图像最相近的k个训练图像，然后根据投票规则，预测测试图像的标签值。特例，当k=1时，就变成最近邻分类器。直观上，较高的k值具有平滑效果，会使得分类器对异常值更具抵抗性。上图中，有色区域代表的是根据L2距离分类器做出的决策边界。在最近邻中，异常值由于可能不正确的预测而导致出现一些与周围颜色不一致的“岛屿”，5-近邻中却将这些异常值做了相应处理，使得这些颜色边界更加平滑，这可能会产生对测试数据更好的泛化。右图中的灰色区域是由投票规则所导致的的（2票红色，2票蓝色，最后一票绿色）。 Validation sets for Hyperparameter tuningHyperparameterk近邻分类器中的k值该如何选择，才会让分类器效果最优。之前距离函数的选择，包括L1、L2范数，这些选择都被称为“超参数”。 一个最简单的想法是，我们尝试许多不同的值，然后看它们中哪一个会让分类器效果更好。但是这不能通过使用测试集来调整超参数。1Evaluate on the test set only a single time, at the very end. Validation sets所以换一种方式去调整超参数：将训练集划分成两个集合，一个是稍微变小的训练集，另一个则是验证集。用CIFAR-10来做个栗子，我们可以使用训练集中的49000张图像来训练，剩下的1000张图像用来验证。验证集就好比是一个伪测试集用来对超参数进行调优的。 对CIFAR-10进行验证集划分：123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) Cross-validation有时候我们的训练集的规模会比较小，这时候我们可以使用一个叫交叉验证的常用方法。比如说，5折交叉验证中，我们将训练集划分为5个相等数量的部分集合，使用其中的4个组成训练集，剩下的一个称为验证集。之后我们迭代选择其中一个为验证集，评估每一次的性能，最后对这5次评估取平均值。上图是k近邻分类器使用5折交叉验证的样例。对于k的每一个值，都是训练4个子集合组成的训练集，在第5个子集合上评估。因此，每个k值都得到了5个准确度值。图中的折线是由每个k的平均准确度相连得到。图中表明，在交叉验证的情况下，当k=7时，准确度相对最高（峰值）。 实际应用中，由于交叉验证在计算上消耗太大，人们一般都偏向于选择将训练数据中的50%-90%作为训练集，其余的作为验证集。但是若是你的训练样本非常少，建议使用交叉验证。常见的交叉验证的折数可以是3折、5折或10折。 Pros and Cons of Nearest Neighbor classifier最近邻：优点： 易于实现和理解 不需要时间去训练，因为所需要做的只是存储和对训练数据进行索引 缺点： 测试时间中计算消耗太大，因为需要去将测试图像与每一个训练图像进行比较。测试时间的效率比训练时更重要。 计算复杂度 虽然说在某些情况下，最近邻的确是一个很好的选择，但是它却很少被应用于实际图像分类环境中去。其中的一个问题便是，图像是一个高维对象（通常包含许多像素），在这高维空间中距离很难被直观地表现出来。 下图便阐明了由之前基于L2距离的相似与感官上的相似有很大不同。最左侧是原图，右侧三幅图分别对原图做了一些变化。然而这三幅图在L2像素距离上与原图有很大的差值。因此，像素级上的距离与感官或语义上的相似完全不相符。 Summary 图像分类，训练集，测试集 最近邻分类器，超参数 验证集 交叉验证 评估 L1、L2距离]]></content>
      <categories>
        <category>cs231n</category>
      </categories>
      <tags>
        <tag>Image Classification</tag>
        <tag>kNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git Bash上传项目命令]]></title>
    <url>%2F2018%2F01%2F13%2Fgit-note%2F</url>
    <content type="text"><![CDATA[记录下平时将项目demo上传至GitHub Repository上的命令。 首先是打开需要上传的项目，切换至该文件夹下，右击Git Bash Here，在Git命令行界面中依次输入以下指令： 将当前文件夹初始化为GitHub项目文件夹：1git init 添加项目中README.md文件，可添加内容备注项目信息等：1git add README.md 设置项目提交后文件上显示的备注信息：1git commit -m &quot;first commit&quot; 添加远程项目仓库：1git remote add origin https://github.com/asdfv1929/test.git 最后push项目到仓库master分支：1git push -u origin master]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全书网小说内容爬取]]></title>
    <url>%2F2018%2F01%2F13%2Fquanshu-novel-net-spider%2F</url>
    <content type="text"><![CDATA[本篇主要记录的是关于小说网的简单爬取，并将内容保存至txt文件中。 前期准备准备工作的话，一方面Python语言，另一方面就是几个Python库的使用：requests、bs4等。关于这些库的具体用法，我这边就不描述了，详细的内容可见官方文档或某些博客：requests：http://docs.python-requests.org/zh_CN/latest/user/quickstart.htmlbs4：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html 小说网链接：http://www.quanshuwang.com/list/1_1.html 小说模型我是定义了一个小说类模型，用来存储或操作一篇小说的相关信息。 首先是，通过小说的一些数据信息对小说进行初始化：12345678def __init__(self, title=None, author=None, cover=None, desc=None, novel_url=None, chapters_url=None): self.NovelTitle = title # 小说名 self.Author = author # 小说作者 self.CoverImgUrl = cover # 封面图片链接 self.Desc = desc # 小说内容的简短介绍 self.NovelUrl = novel_url # 小说跳转链接 self.ChaptersUrl = chapters_url # 小说目录链接 self.Chapters = [] # 小说所有章节(chapterTitle, chapterUrl)列表 之后，我简单定义了一个获取网页源码的方法：12345# 读取传入参数url的网页源码 def readHtml(self, url): html = requests.get(url) # 请求url html.encoding = 'gbk' return html.text 接着便是获取该小说所有章节的标题及章节链接：1234567# 获取该小说所有章节的(title, url)组成的列表Chapters def getChaptersUrl(self): html = self.readHtml(self.ChaptersUrl) soup = BeautifulSoup(html, 'html.parser') for chapter in soup.select(".chapterNum ul li a"): self.Chapters.append((chapter.string, chapter['href'])) # 章节名及链接加入列表 return self.Chapters 最后便是通过章节链接，去访问该章节对应的小说内容，当然，里面还包括了将内容存入本地txt：1234567891011121314151617# 获取该小说某一章节的小说文本内容，并写入文件 chapter: (chapter_title, chapter_url) def getOneChapterContent(self, chapter): k = 0 obj = None while k &lt; 3: # 容错 html = self.readHtml(chapter[1]) soup = BeautifulSoup(html, 'html.parser') obj = soup.find(attrs="mainContenr") if obj: break k += 1 if not obj: print('None') return content = obj.text content_drop = content.replace("style5();", "").replace("style6();", "").replace('&lt;br /&gt;', '\n').replace(u'\xa0', u' ').replace(u'\ufffd', u' ') self.write2txt(chapter[0], content_drop) 其中，里面有调用了写入文本的方法：12345# 写入txt，传入章节名和该章节对应的文本内容 def write2txt(self, chapter_name, content): with open('Novels/' + self.NovelTitle + '.txt', 'a') as file: print(self.NovelTitle + ' 正在写入 ' + chapter_name) file.writelines(chapter_name + '\n\n' + content + '\n\n') 所有小说信息在这边，我主要是获取链接页面中，所有小说的基本信息，例如小说名、作者、封面图片链接、内容简介、跳转链接等。首先便是，基本的网页源码读取：12345# 返回页面源码def readHtml(url): html = requests.get(url) # 请求url html.encoding = 'gbk' return html.text 接着是获取该页面上所有小说的基本信息：小说名、作者、封面链接、内容描述、跳转链接，返回信息列表123456789101112131415# 获取小说信息def get_novels_info(url): novels = [] html = readHtml(url) soup = BeautifulSoup(html, 'html.parser') for item in soup.select(".seeWell li"): title = item.find(attrs='clearfix').text # 小说名 author = item.span.find_all('a')[1].text # 作者 coverImg = item.find('img')['src'] # 小说封面链接 desc = item.find('em').text.replace('\n', '').replace(' ', '') # 内容描述 novelUrl = item.span.find('a')['href'] # 小说跳转链接 #print(title, author, coverImg, desc, novelUrl) novel = [title, author, coverImg, desc, novelUrl] novels.append(novel) return novels 这边是小说信息写入csv文件的方法：123456# 将小说信息写入csv文件def write2csv(novels): with open("novels.csv", 'a', newline='') as file: writer = csv.writer(file) for novel in novels: writer.writerow(novel) 之后便是获取小说跳转至目录的链接，并将该链接也添加至数据信息列表，并返回：123456789101112131415161718# 获取小说跳转至章节的链接def get_chapters_url(novel_list): for novel in novel_list: k = 0 obj = None while k &lt; 3: html = readHtml(novel[-1]) soup = BeautifulSoup(html, 'html.parser') obj = soup.find(attrs="reader") if obj: break k += 1 if not obj: novel.append('') continue chapters_url = obj['href'] novel.append(chapters_url) return novel_list 最后便是实例化小说对象，然后获取所有章节链接，之后遍历访问它的小说内容：1234567def all_chapters_url(novel_list): for novel in novel_list: a_novel = ANovel(novel[0], novel[1], novel[2], novel[3], novel[4], novel[5]) # 初始化一个novel对象 a_novel.getChaptersUrl() # 获取该小说对象的所有章节的链接 #print(a_novel.Chapters[:10]) # 打印小说章节列表中的前10章 for chapter in a_novel.Chapters: # 遍历该小说的所有章节 a_novel.getOneChapterContent(chapter) # 获取某一章节的文本内容 调用运行最后就是在main里调用运行即可：1234567if __name__ == '__main__': url = 'http://www.quanshuwang.com/list/1_%s.html' for page in range(1, 2): # range范围可修改 novelsObjList = get_novels_info(url % page) novelsObjList = get_chapters_url(novelsObjList) # write2csv(novelsObjList) all_chapters_url(novelsObjList) 运行显示： 小说下载： 附上程序：https://github.com/asdfv1929/QuanshuNetNovelSpider]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>novel</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow实现Softmax Regression识别手写数字]]></title>
    <url>%2F2018%2F01%2F05%2Ftf-softmax-mnist%2F</url>
    <content type="text"><![CDATA[这是本人在学习TensorFlow实战一书时所记录下来的一些内容。 MNIST数据集MNIST（Mixed National Institute of Standards and Technology database）是一个非常简单的机器视觉数据集，如上图所示，它由几万张28像素x28像素的手写数字图片组成，这些图片只包含灰度值信息（channel=1）。我们所需要做的任务就是对这些手写数字的图片进行分类，转成0~9一共10类。 首先对MNIST数据进行加载，TensorFlow为我们提供了一个方便的封装，可以直接加载MNIST数据成我们期望的格式。在Jupyter上运行代码：12345from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data", one_hot=True)print(mnist.train.images.shape, mnist.train.labels.shape) print(mnist.test.images.shape, mnist.test.labels.shape) print(mnist.validation.images.shape, mnist.validation.labels.shape) 结果显示为：1234567891011Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.Extracting MNIST_data\train-images-idx3-ubyte.gzSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.Extracting MNIST_data\train-labels-idx1-ubyte.gzSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.Extracting MNIST_data\t10k-images-idx3-ubyte.gzSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.Extracting MNIST_data\t10k-labels-idx1-ubyte.gz(55000, 784) (55000, 10)(10000, 784) (10000, 10)(5000, 784) (5000, 10) 数据集成功下载获得，并打印出mnist的训练集中有55000张图像，784维的特征，测试集有10000张图像，验证集有5000张图像。 注：1) 图像是28像素x28像素大小的灰度图片，即空白部分全部为0，有笔迹的地方根据颜色深浅有0到1之间的取值。2) 每个样本有784维的特征，来自于将28x28个像素点展开成一维的结果（28x28=784）。3) 此处简化了问题，丢弃图像的空间结构的信息，将图片按同样的顺序展开至1维向量即可。4) 训练数据的特征是一个55000x784的Tensor，第一个维度是图片的编号，第二个维度是图片中像素点的编号。同时训练的数据Label是一个55000x10的Tensor。测试集、验证集同样道理。5) 读取数据时，对10个种类进行了one-hot编码，Label是一个10维的向量，只有1个值为1，其余为0。比如数字0，对应的Label就是[1,0,0,0,0,0,0,0,0,0]，数字5对应的Label就是[0,0,0,0,0,1,0,0,0,0]，数字n就代表对应位置的值为1。 Softmax Regression数据准备好后，我们采用的是一个叫Softmax Regression的算法来训练手写数字识别的分类模型。数据集的数字都是0~9之间的，所以一共有10个类别。当模型对一张图片进行预测时，Softmax Regression会对每一种类别估算一个概率，比如预测是数字3的概率为80%，是数字5的概率为5%，最后取概率最大的那个数字为模型的输出结果。 $i$代表第$i$类，$j$代表一张图片的第$j$个像素，$b_i$是bias。$$feature_i = \sum_j W_ix_j + b_i$$接下来对所有特征计算Softmax，即计算一个exp函数，然后再进行标准化（让所有类别输出的概率值之和为1）。$$softmax(x) = normalize(exp(x))$$其中判定为第$i$类的概率可由下面公式得到：$$softmax(x)_i = \frac{exp(x_i)}{\sum_jexp(x_j)}$$ 整个Softmax Regression的流程如下图所示：转换为公式的话，如下图所示，将元素相乘变成矩阵乘法：即：上述矩阵表达写成公式的话，就可用下面一行表达：$$y = softmax(Wx + b)$$ TensorFlow实现12345import tensorflow as tfsess = tf.InteractiveSession() # 将InteractiveSession注册为默认的session，之后的运算默认跑到这个session里# 不同session之间的数据和运算都是相互独立的x = tf.placeholder(tf.float32, [None, 784]) # 创建placeholder，输入数据。第一个参数是数据类型，第二个参数是tensor的数据尺寸，这里None代表不限制条数的输入，784维向量 12W = tf.Variable(tf.zeros([784, 10])) # Weights参数，Variable在模型训练迭代中式持久化的。weights初始化为0b = tf.Variable(tf.zeros([10])) # bias全部初始化为0 注：1) 存储数据的tensor一旦使用就会消失，但Variable在模型训练迭代中是持久化的，可长期存在并在每轮迭代中被更新。2) 这里全部初始化为0，因为模型训练时会自动学习合适的值，对这个简单模型来说初始值不太重要。3) 但对于复杂的卷积网络、循环网络或较深的全连接网络，初始化得方法比较重要。123# y = softmax(Wx + b) 改写成TensorFlow语言y = tf.nn.softmax(tf.matmul(x, W) + b) # 实现Softmax算法# tf.matmul 矩阵乘法函数 1234y_ = tf.placeholder(tf.float32, [None, 10]) # 输入真实的label的概率分布 y_# 损失函数cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])) # reduce_sum求和的cgama， reduce_mean对每个batch数据结果求均值 注：1) 损失函数越小，代表模型的分类结果与真实值的偏差越小，即模型越精确。2) 训练的目的是不断将这个损失减小，直到达到一个全局最优或者局部最优解。3) 对多分类问题，通常使用cross-entropy交叉信息熵来作为损失函数。123# 定义一个优化算法，设置学习率，设定优化目标为cross-entropy# 常用 随机梯度下降SGD算法train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) # 学习率0.5 12# 使用TensorFlow的全局参数初始化器tf.global_variables_initializer，并直接运行run方法tf.global_variables_initializer().run() 12345# 迭代训练操作train_step# 每次随机从训练集中抽取100条样本构成一个mini-batch，并feed给placeholder，调用train_step进行训练for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) train_step.run(&#123;x: batch_xs, y_:batch_ys&#125;) 1correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) 注：argmax获取的是最大值所在位置索引，所以一个是y中概率最大的那类所在位置索引，另一个是真实值y_中最大（即值为1）的位置索引，之后将两者进行equal比较，相等则为true，反之false，最后获得correct_prediction的bool矩阵。1234# 统计全部样本预测的accuracy，先用cast将之前的correct_prediction输出的bool值转换为float32，再求平均# bool转float，即true为 1.0，false为 0.0# 用reduce_mean计算平均值，即为精度值：相等（1.0，即预测正确）的数量在总数量中的占比accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 12# 将测试数据的特征和label输入评测流程accuracy，计算模型在测试集上的准确率print(accuracy.eval(&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 最后的预测精度约为92% 。 附上程序：https://github.com/asdfv1929/TensorFlowInActionLearning (TensorFlow实现Softmax Regression识别手写数字)]]></content>
      <categories>
        <category>TensorFlow实战学习</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>TensorFlow</tag>
        <tag>MNIST</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Happy New Year]]></title>
    <url>%2F2018%2F01%2F01%2Fnew-year-2018%2F</url>
    <content type="text"><![CDATA[今天是2018年1月1日，新一年的第一天，不写些东西，还真有点不应这个元旦的景。 所谓是新年新气象，希望接下来的一年，能够尽快地完成自己的目标。虽然我知道，这个过程是有点难受，不过不打紧，不停顿、保持往前走的态势就行，希望能够有所收获。 之后希望我的家人，还有大家都平平安安、幸福美满。 asdfv19292018.1.1 14:55]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[翻译：ImageNet Classification with Deep Convolutional Neural Networks]]></title>
    <url>%2F2017%2F12%2F31%2Falexnet%2F</url>
    <content type="text"><![CDATA[此为本人在学习AlexNet时，将论文翻译成中文而作，以翻促学。论文地址：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 注：本人也是第一次翻译外文论文，若有纰漏之处，敬请谅解，并可邮箱通知于我[asdfv1929#163.com]进行修改；翻译内容仅供学习使用。 Name DESC TITLE ImageNet Classification with Deep Convolutional Neural Networks AUTHOR Alex Krizhevsky PBYEAR 2012 TRANSLATION asdfv1929 TIME 2017.12 正文翻译： 摘要我们训练了一个大规模的深度卷积神经网络去将ImageNet LSVRC-2010比赛中的120万张高清图像划分到1000个不同的类别中。在测试数据上，我们将top-1和top-5的误差率分别降到了37.5%和17.0%，这比之前的技术水平要好得多。该神经网络，拥有6千万个参数（parameters）和65万个神经元（neurons），包含有5个卷积层（convolutional layers），其中一些卷积层的后面跟着最大池化层（max-pooling layers），还有3个全连接层（full-connected layers）以及一个最后的1000-way softmax函数。为了让训练速度更快，我们采用了非饱和神经元（non-saturating neurons）和一种卷积操作的高效GPU实现方法。为减少全连接层中的过拟合，我们采用了一个最近研究出来的正则化方法（regularization method），叫“dropout”，它被证明是十分有效的。我们也用该神经网络模型的一个变种去参加了ILSVRC-2012比赛，并且同第二名的top-5误差率26.2%相比，我们以top-5误差率15.3%获得了冠军。 引言当前的目标识别方法主要都是利用了机器学习方法。为了提高他们的性能，我们可以收集更大规模的数据集，学习更多强大的模型，采用更优的技术来防止过拟合（overfitting）。直到最近，带有标签（label）的图像数据集的规模仍是相对较小，一般是在万张数量级上（例如NORB，Caltech-101/256，和CIFAR-10/100）。我们可以在这种规模大小的数据集上做简单的识别任务，特别是当它们通过标签保存转换（label-preserving transformations）方法增强了数据。举例来说，当前在MNIST数字识别任务上的最优误差率（&lt;0.3%）已接近人的表现。但是现实环境中的目标对象（object）有着相当大的变化性，所以若要学习去识别它们，就有必要使用更大规模的训练集。事实上，小规模图像数据集的缺点早已被公认，但到最近才可能收集到数百万张带标签的图像数据集。这些新的更大规模的数据集包括有LabelMe，它由数十万张全分割（full-segmented）的图像组成，还有ImageNet，它由超过1500万张带有label的高清图像组成，这些图像有超过22000个种类。 为了从百万张图像中学习数千种目标，我们需要一个具有较大学习能力的模型。然而，目标识别任务的巨大复杂性意味着即使像ImageNet这等规模大的数据集也不能确定该问题，所以我们的模型应采用许多先验知识来弥补我们所没有的数据。卷积神经网络（CNNs）便构成了这样的一类模型。它们的能力可以通过改变它们的深度（depth）和广度（breadth）来控制，并且它们对图像的性质（即，统计上的稳定性和像素依赖的局部性stationarity of statistics and locality of pixel dependencies）也能做出强大且大多正确的假设（预测）。因此，同具有相似规模层（similarly-sized layers）的标准前馈（feedforward）神经网络相比，CNNs拥有更少的连接和参数，因此它们更易于训练，而且它们理论上的最优性能可能仅比前馈神经网络稍差一些。 尽管CNNs具有吸引人的一些特性，而且对于本地架构（local architecture）非常的高效，但它们在大规模应用于高分辨率图像方面上仍然过于昂贵。幸运的是，当前的GPUs，加上二维卷积的高度优化实现方法，足以促进有趣的CNNs的训练，最近的数据集比如说ImageNet，包含了足够的标记样本（labeled examples）来训练此类模型，且不会出现严重的过拟合问题。 本文的主要贡献如下所示：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了最大的卷积神经网络之一，并且在这些数据集上获得了迄今为止最好的结果。我们编写了一个二维卷积的高度优化的GPU实现方法，以及其他所有在训练神经网络过程中固有的一些操作，这些我们都公开提供（ http://code.google.com/p/cuda-convnet/ ）。我们的网络包含了一些新的和不寻常的特点，它们可以提高网络的性能，缩短训练时间，具体内容见 Section 3。即使是拥有120万个带标记的训练样本，网络的大小仍然会使得过拟合（overfitting）成为一个严重的问题，所以我们使用了几个有效的技术来防止过拟合，具体信息将在Section 4中介绍。我们最终的网络包含了5个卷积层和3个全连接层，其中深度看上去很重要：我们发现，若移除任意一个卷积层（每个卷积层仅包含不到1%的模型参数）均会导致性能变差。 最后，网络的规模大小主要受限于当前GPUs的可用存储量以及我们能接受的训练时间。我们的神经网络在两台GTX 580 3GB GPUs上训练花费了5至6天的时间。我们所有的实验均表明，只要有更快的GPUs和更大的数据集，我们的实验结果就能进一步提高。 数据集ImageNet是一个拥有超过1500万张带标签的高分辨率图像的数据集，且这些图像大致归属于22000个类别。这些图像收集自网络，并由human labelers使用Amazon的Mechanical Turk crowd-sourceing tool进行人工标记。从2010年开始，作为Pascal Visual Object Challenge的一部分，一个叫ImageNet Large-Scale Visual Recognition（ILSVRC）的比赛开始举办。ILSVRC使用了ImageNet的子集，这个子集中包含了1000个类别，每个类别大约有1000张图像。总之，这个子集大概有120万张训练图像，5万张验证图像，以及15万张测试图像。 ILSVRC-2010是ILSVRC中测试集labels可获得的唯一版本，因此我们是在这个数据集上做了大多数实验。我们也用我们的模型参加了ILSVRC-2012比赛，在Section 6我们会展示关于这个数据集（2012）的实验结果，但其测试集的labels不可获得。在ImageNet中，通常检验这两类误差率：top-1和top-5，其中top-5误差率表示测试图像中的正确label不在模型所认为的可能性最大的5个labels当中的占比。 ImageNet包含了各种分辨率的图像，而我们的系统要求输入数据的维度恒定（constant input dimensionality）。因此，我们对图像进行下采样（down-sample）到一个固定的分辨率 256 x 256。给定一个矩形图像，我们首先缩放图像使得图像的短边长度为256，之后从结果图像中裁剪出中心 256 x 256 大小的块。我们并未使用任何其他方法对图像进行预处理，除了从每个像素中减去训练集的平均活动（subtracting the mean activity over the training set from each pixel）。因此我们是在像素的原始RGB值上训练我们的网络模型。 架构我们网络的架构在图2中总结展示出。它包含了8个学习层–5个卷积层和3个全连接层。接下来，我们将介绍我们网络的架构中几个新的不寻常的特点。Sections 3.1-3.4按照我们对它们重要性的评估进行排序，最重要的排在前列。 ReLU 非线性（Nonlinearity）对神经元输出 f 作为其输入 x 的函数的标准建模方法是 f(x) = tanh(x) 或 f(x) = (1 + e^-x)^-1 。从采用梯度下降（gradient descent）方法的训练时间来看，这些饱和非线性（saturating nonlinearities）是比非饱和非线性（non-saturating nonlinearity）f(x) = max(0, x) 要慢得多。根据Nair和Hinton的想法，我们把具有这种非线性的神经元称为整流线性单元Rectified Linear Units （ReLUs）。使用ReLUs的深度卷积神经网络比使用单元的网络训练速度快上几倍。这在Figure 1中可以看到，上面展示了在一个特定4层卷积网络上对CIFAR-10数据集的训练误差率降到25%所需要的迭代次数。这幅图表明，如果我们使用的是传统的饱和神经元模型，我们将不能够训练出如此大规模的神经网络。 Figure 1：一个采用ReLUs（实线）的4层卷积神经网络达到训练误差率25% 6倍快于带有tanh神经元（虚线）的同等网络。每个网络的学习率都是独立地进行选择以求尽可能快地训练。网络中没有采用任何类型的正则化。这里所展示的效果的量级受网络架构的影响而有可能不同，但是具有ReLUs的网络一直是比带有饱和神经元的网络在学习速度上要快上几倍。 我们并不是第一个考虑在CNNs中替换传统神经元模型的人。就比如说，Jarrett等人声称，非线性 f(x) = |tanh(x)| 在Caltech-101数据集上与局部平均池化（local average pooling）后的对比归一化的合作表现良好（the nonlinearity f(x) = |tanh(x)| works particularly well with their type of contrast normalization followed by local average pooling on the Caltech-101 dataset.）。然而，他们在这数据集上主要关注的是防止过拟合，所以观察到的结果与我们报告中使用ReLUs拟合训练集的加速能力有所不同。更快速的学习能力对在大规模数据集上训练的大型模型的性能具有很大的影响。 在多个GPUs上训练单个GTX 580 GPU只有3GB的内存大小，这限制了能够在其上训练的网络的最大规模。事实证明，120万个训练样本足以训练出网络，但这对于单个GPU来说太大了。因此，我们将网络分布到两个GPUs上。当前的GPUs非常适合跨GPU做并行计算，因为它们能够直接向另一个GPU做读取写入操作，而无需通过主机内存中转。我们所采用的的并行化方案基本上是在每个GPU上放置一半的内核（或神经元），另外还有一个技巧：GPUs之间的通信只在某些层中进行。这意味着，比如说，第3层的内核从第2层的所有内核映射中获取输入。然而，第4层的内核只从第3层中和自己在同一个GPU上的内核映射中获取输入。选择连接的模式对于交叉验证来说是一个问题，但我们可以精确地调整通信量，直到它达到计算量的可接受部分为止。 由此产生的架构有点类似于Cirespan等人使用的“柱状（columnar）”CNN，只是我们的纵列（columns）不是独立的。与在一个GPU上训练的且每个卷积层内核数量减少一半的网络相比，这个方案将我们的top-1和top-5误差率分别降低了1.7%和1.2%。训练双GPU网络的时间较少于单个GPU网络。 局部响应归一化（Local Response Normalization）ReLUs有一个特性使得它们不需要输入归一化来防止它们饱和。如果有一些训练样本产生了正输入（positive input）给ReLUs，就会在那个神经元中进行学习操作。然而，我们仍发现下面这种局部归一化方案有助于一般化（generalization）。假设用 αix,y 表示在 (x, y) 处由第 i 个内核计算而得的神经元的活动，之后应用ReLU非线性，最后响应归一化活动 bix,y 由以下公式定义：其中求和操作是作用于同一空间位置的n个邻近内核映射上，N是层中内核总数。内核映射的顺序是任意的，且在训练开始前就确定好了的。受到真实神经元中的类型所启发，这种响应归一化实现了一种侧向抑制形式（a form of lateral inhibition），为使用不同内核计算得到的神经元输出中的大型活动（big activities）创建竞争机制。常量 k,n,α,β 是超参数，它们的值通过验证集来确定；我们取k=2, n=5, α=10-4, β=0.75 。在特定层应用ReLUs非线性后，我们应用了这种归一化（见Section 3.5）。 该方案与Jarrett等人的局部对比归一化（local contrast normalization）方法有一些相似之处，但是我们的方案更应正确地被命名为“亮度归一化（brightness normalization）”，因为我们没有减去平均活动。响应归一化将我们的top-1和top-5误差率分别下降了1.4%和1.2%。我们也在CIFAR-10数据集上验证了该方案的有效性：未采用归一化的4层CNN取得了13%的测试误差率，采用归一化的只有11%。 重叠池化（Overlapping Pooling）CNNs中的池化层（Pooling layers）汇总了在同一内核映射中相邻神经元组的输出。传统上，由邻近池化单元汇总的邻近关系不会重叠。更准确地说，一个池化层可以被认为是由间隔 s 像素的池化单元组成的网格，每个网格均汇总出以池化单元的位置为中心的大小为 z x z 的邻域关系。如果我们设定 s=z，我们将得到CNNs中常用的传统的局部池化。如果我们设定 s&lt;z，我们将得到重叠池化。这就是我们在网络中所使用的，其中 s=2， z=3。与无重叠的方案 s=2， z=2 相比，这种方案在产生相同维度的输出时分别将top-1和top-5的误差率降低了0.4%和0.3%。我们还观察到，采用重叠池化训练模型会使模型不易出现过拟合问题。 总体架构现在我们开始介绍我们CNN的总体架构。如图2所示，网络中包含了8个加权（weights）的层；前5个是卷积层，余下的3个是全连接层。最后一个全连接层的输出被发送给1000-way softmax上，其产生1000个类别标签的分布。我们的网络使得多项式逻辑回归目标（multinomial logistic regression objective）最大化，这相当于最大化了预测分布下训练样本中正确标签的log概率的平均值。 第2、4、5个卷积层的内核只连接到同一GPU上前一层的那些内核映射上（见Figure 2）。第3个卷积层的内核连接到第2个卷积层的所有内核映射上。全连接层中的神经元与前一层的所有神经元相连。响应归一化层跟在第1、2个卷积层后面。Section 3.4中描述的那种最大池化层跟在两个响应归一化层和第5个卷积层的后面。ReLUs非线性被应用于每个卷积层的输出和全连接层的输出上。 第1个卷积层利用96个大小为 11 x 11 x 3 的内核，采用步长4个像素（同一内核映射中相邻神经元的感受野中心之间的距离），对 224 x 224 x 3 的输入图像做滤波（filter）处理。第2个卷积层将第1个卷积层的（响应归一化、池化后的）输出作为输入，且利用256个大小为 5 x 5 x 48 的内核进行滤波。第3、4、5个卷积层彼此相连，中间没有任何的池化层或归一化层。第3个卷积层有384个大小为 3 x 3 x 256 的内核连接到第2个卷积层的输出上。第4个卷积层有384个大小为 3 x 3 x 192 的内核，第5个卷积层有256个大小为 3 x 3 x 192 的内核。全连接层都各有4096个神经元。 Figure 2：CNN架构的图解，明确地展示出两GPU间的责任划定。一个GPU运行图画中顶部的每层部分，另一个运行图画中底部的每层部分。GPUs间的通信只在某些层中进行。网络的输入是150,528维，网络中剩余层的神经元数量分别是：253,440–186,624–64,896–64,896–43,264–4096–4096–1000。 减少过拟合我们的神经网络架构中拥有6000万个参数。虽然ILSVRC的1000个类别使得每个训练样本在从图像映射到标签label时都强加了10bits的约束（impose 10 bits of constraint on the mapping from image to label），但是这不足以在学习如此多的参数情况下而没有大量的过拟合（在学习这么多的参数情况下必定会有过拟合问题）。下面，我们介绍两个对抗过拟合的主要方法。 数据增强（Data Augmentation）在图像数据上减少过拟合的最简单和最常见的方法是使用标签保存转换（label-preserving transformations）方法人为地扩大数据集规模。我们采用了两个不同的数据增强方式，两者都允许以少量的计算从原始图像中生成转换图像，所以转换图像不需要存储在硬盘上。在我们的实现中，转换图像是由CPU上的Python代码生成的，而GPU正在训练前一batch的图像。因此这些数据增强方案事实上是计算自由的（computationally free）。 第一种数据增强方案包括了生成图像翻译（image translations）和水平反射（horizontal reflections）。为此，我们从 256 x 256 的图像中提取随机的 224 x 224 区块（patches）（和它们的水平反射图像），并在这些提取出来的区块上训练我们的网络。这使得我们的训练集增加了2048倍（（256-224）^2 * 2），尽管由此产生的训练样本相互之间高度关联。若没有这种方案，我们的网络将遭受严重的过拟合问题，这就会迫使我们采用更小规模的网络。在测试过程中，网络通过提取5个 224 x 224 区块（4个角落区块和一个中央区块）和它们的水平反射（因此共有10个区块）来做预测，并且对由网络softmax层对这10个区块的预测值做平均处理。 第二种数据增强方案包括了更改训练图像的RGB通道的强度（altering the intensities of the RGB channels）。具体来说，我们在整个ImageNet训练集上的RGB像素值集合上都采用了PCA方法。对于每个训练图像，我们成倍增加已有的主要成分，比例大小为对应特征值乘上一个从均值0标准差0.1的高斯分布中提取的随机变量。因此对于每个RGB图像像素，我们添加如下：其中是RGB像素值的 3 x 3 协方差矩阵的第 i 个特征向量和特征值，αi是前面提到的随机变量。每个αi对于一个特定的训练图像的所有像素仅被绘制一次，直到该图像被再次用来训练，此时它才会被再次绘制。这种方案近似捕捉到了自然图像的一个重要特性，即，目标身份不会随光照的强度和颜色的变化而改变。该方案将top-1误差率降低了1%。 Dropout结合众多不同模型的预测是一个减少测试误差的不错的方式，但是这对于大型神经网络来说仍过于昂贵，得花上几天时间来训练。然而，有这么一个高效的模型组合版本，只花费两倍的在单个模型上的训练时间。该最近引入的技术，称为“dropout”，以0.5的概率将每个隐藏神经元的输出置为0。以这种方式被“drop out”的神经元既不对前向传播（forward pass）做贡献也不参与反向传播（backpropagation）。所以每次提交输入时，神经网络都采用不同的架构，但所有的架构共享权值。这种技术降低了神经元复杂的互适应关系，因为一个神经元不能依赖于其他特定神经元的存在。因此，它被迫学习更多健壮的特征，这些特征在与其他神经元的不同随机子集相连时是非常有用的。在测试时，我们使用所有的神经元，但它们的输出乘以0.5，对于获取指数级dropout网络产生的预测分布的几何平均值，这是一种合理的近似（a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks）。 我们在Figure 2的前两个全连接层上采用了dropout。若没有dropout，我们的网络会表现出实质性的过拟合。Dropout使收敛所需的迭代次数大致增加了一倍。 学习的细节我们采用了随机梯度下降（stochastic gradient descent）方法，并令batch size = 128，momentum = 0.9，weight decay = 0.0005，对模型进行训练。我们发现这个小量的weight decay对于模型学习是十分重要的。换句话说，这儿的weight decay不仅仅是一个regularizer：它减少了模型的训练误差。权值weight w的更新规则是：其中 i 是迭代索引，v 是momentum变量，ε是学习率，是第 i 个batch Di上的目标L在 wi 处对 w 的偏导数的平均值。 我们用一个均值为0、标准差为0.01的高斯分布初始化了每一层的权值。我们将第2、4、5个卷积层和全连接隐层的神经元偏移量biases初始化为常量1。这种初始化通过向ReLUs提供正输入来加速学习的早期阶段。 我们将其余层中的神经元偏差biases初始化为0。 我们对所有图层使用了相同的学习率，它是由我们在训练过程中手动调节出来的。我们所遵循的启发式是当验证误差率在当前学习率下不再提升（数字下降）时，就将学习率除以10。学习率初始化为0.01，且在训练终止前下降3次。我们用120万张图像的训练集对网络进行了大约90个周期的训练，在两台NVIDIA GTX 580 3GB GPU上花费了五到六天的时间。 结果关于ILSVRC-2010数据集的结果已总结展示在表1中。我们训练的网络在top-1和top-5测试集误差率上分别是37.5%和17.0%。在ILSVRC-2010比赛中取得的最好结果是47.1%和28.2%，它是通过由不同特征训练出来的6个稀疏编码（sparse-coding）模型产生的预测值求平均而得；之后，所公布的最好结果是45.7%和25.7%，它是在由两种类型密集采样特征（densely-sampled features）计算得到的Fisher向量（Fisher Vectors）上训练出来的分类器的预测值求平均。 我们同样将我们的模型参加了ILSVRC-2012比赛，其中获得的结果如表2所示。由于ILSVRC-2012测试集的labels并未公开提供，因此我们不能报告展示出我们尝试的所有模型的测试误差率。在本段的剩余部分，我们使用验证误差率来替换测试误差率，因为根据我们的经验，它们之间的差异不会超过0.1%（见表2）。本文介绍的CNN其top-5误差率达到了18.2%。5个相似CNN的top-5误差率平均值是16.4%。训练一个在最后池化层后加上第6个卷积层的CNN，去对整个ImageNet Fall 2011版本数据集（1500万张图像，22000个类别）进行分类，然后对其进行调优，在ILSVRC-2012上的top-5误差率达到16.6%。两个对Fall 2011数据集预训练的CNN，加上前面所提到的5个CNN，对它们的预测值求平均后，top-5误差率达到15.3%。比赛中第二名的top-5达到26.2%，他的方法是在几个不同类型的密度采样特征（densely-sampled features）计算得到的FVs上训练出的分类器的预测值的平均值。 最后，我们也展示了在ImageNet Fall 2009版本数据集（10184个类别，890万张图像）上的误差率。在这个数据集上，我们遵循文献中使用一半图像做训练和一半做测试的惯例。由于没有建立测试集，我们的数据集划分应该与之前的作者使用的划分有所不同，但是这并不影响结果。在这数据集上的top-1和top-5误差率达到67.4%和40.9%，该结果是由上述网络（在最后池化层后带有一个额外的第6个卷积层）所得到。在该数据集上已公布的最优结果是78.1%和60.9%。 定性评估（Qualitative Evaluations）Figure 3展示了通过网络的两个数据连接层学习得到的卷积内核。该网络已学习了各种频率选择（frequency-selective）和方向选择（orientation-selective）的内核，以及各种色块斑点（colored-blobs）。考虑到由两个GPUs展示的专业化，受限连通性（restricted connectivity）的结果呈现在Section 3.5中。GPU 1上的内核很大程度上与颜色无关（ color-agnostic），而GPU 2上的内核与颜色密切相关（ color-specific）。这种专业化发生在每一次运行当中，并且是独立于任何特定的随机权重初始化（GPUs重新编号）。 Figure 3：由第1个卷积层在 224 x 224 x 3 输入图像上训练得到的96个大小为 11 x 11 x 3 的卷积内核。上边的48个内核是在GPU 1上学习得到，下边的48个内核是在GPU 2上学习得到。具体内容见Section 6.1。 Figure 4：（左）8张ILSVRC-2010测试图像和由模型认为的最有可能的5个标签。正确的标签显示在每张图像的下方，分配给正确标签的概率也用红色框标注显示在上图中（如果正确标签在预测的5个标签当中）。（右）第一列上有5张ILSVRC-2010测试图像。其余列上展示的是与测试图像在最后一个隐层上的特征向量具有最短欧氏距离的6张训练图像。 在Figure 4的左半部分，我们通过计算网络模型在8张测试图像上的top-5预测值来定性地评估（网络）学到了什么。请注意，即使是偏离中心位置的目标，比如左上角的那只螨虫，仍被网络所识别出来。大部分top-5的标签（labels）看起来都很合理。比如说，只有一些其他类别的猫科动物被错误认为是豹类。在某些情况下（汽车护栅，樱桃），网络对照片中究竟应关注哪个目标对象存在着歧义。 探索网络视觉内容的另一种方法是考虑由最后一个图像（4096维度的隐藏层）引起的特征激活（feature activations）。如果两张图像的特征激活向量具有较小的欧氏距离，则可以说神经网络认为它们在很大程度上相似。Figure 4右半部分中展示了测试集中的5张图像，还有根据上述方法找出来的与这5张图像中每个图像最相似的6张训练集中的图像。请注意，在像素级别上，所检索到的训练图像一般是不与第一列中的查询图像在L2上相近。比如说，所检索的dogs和elephants图像中它们有各种各样的姿势。我们在补充材料里提供了更多的测试图像的结果。 在两个4096维的实值向量之间使用欧氏距离来计算相似度的效率十分低，但是通过训练自动编码器（auto-encoder）来将这些向量压缩成短二进制码（short binary codes）将会使之变得高效。这应该会产生出一种比对原始像素应用自编码更好的图像检索方法，因为不需要用到图像标签labels，因此倾向于检索出带有相似边缘模式的图像，无论它们在语义上是否相似。 讨论我们的结果展示出：一个大型深度卷积神经网络能够在一个极具挑战的数据集上使用纯监督学习（supervised learning）而取得破纪录的成绩结果。值得注意的是，如果移除一个卷积层，我们网络的性能就会下降。比如，移除任意一个中间层就会导致网络的top-1误差率增加2%。所以深度（depth）对于我们获得结果非常重要。 为了简化实验，我们没有使用任何无监督的预训练，即使我们知道这样会有帮助，特别是当我们获得了足够的计算能力去大幅度提升网络的规模而不去对应地增加带有label的数据时。至此，我们的结果已经得到优化，因为我们已让网络规模更大，训练它们的时间更长，但是我们仍有许多数量级（many orders of magnitude）要去做，以求匹配上人类视觉系统的神经-时间通路（infero-temporal pathway）。最后，我们希望在视频序列上运用大型深度卷积神经网络，因为视频序列的时序结构提供了在静态图像中丢失或者不明显的有用信息。]]></content>
      <categories>
        <category>CNN Papers</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
        <tag>Papers</tag>
        <tag>AlexNet</tag>
        <tag>Translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用GitHub+Node.js+Hexo搭建个人博客]]></title>
    <url>%2F2017%2F11%2F18%2Fhexo-next-blog%2F</url>
    <content type="text"><![CDATA[本篇是自己在搭建Hexo博客平台时的一个过程记录。 GitHub账号注册因为此文所搭建的个人博客是基于GitHub平台服务的，所以首先是注册GitHub，当然已有账号的跳过此步； 传送门：GitHub官网，页面如下，输入username、email、password后点击Sign Up注册，当然这些之后是需要去对应的邮箱邮件里进行注册激活。页面跳转至如下，这边我们是点击Continue，默认里面其他设置。页面如下，根据问题勾选自己对应的。 创建Repository基本配置完成后，跳转至如下，这边我们就直接Start a project了，你也可以选择先看下guide，点击开始后，跳转至Create a new repository，这边Repository name命名规则就是username.github.io，其中username就是你注册时的username。 Settings点击创建之后页面跳转至project内，这时我们点击settings这边就是setting下这时我们在该页面往下拖动网页，找到GitHub Pages，之后点击Choose a theme选择页面主题，这边我们就暂时选择默认的主题（因为后面我会更换为NexT主题的），然后点击Select theme。 GitHub Page点击选择主题后，页面会跳转至该Repository的可以说是主页吧，如下所示。上面有提醒主题更换，也有生成一个index.mdmarkdown文件。这时我们再去刚才的setting设置里去看刚刚的GitHub Pages那边，会有显示你的url，这就是你未来博客“搭建”在该网址上。此时点击该url访问到的也就是你未来个人博客将会展示的样子 Node.Js下载安装传送门：NodeJs，选择对应的版本进行下载，安装的话就点点点，这边就不在叙述了。软件安装完成后，打开cmd界面，输入node -v和npm -v（注意查看环境变量部分是否已经正确），查验其版本，确认是否安装成功。 安装Hexo在命令行输入：1npm install -g hexo-cli 之后创建一个文件夹（搭建博客的相关文件存放于此），此处我命名为test（可自主命名），在该文件夹路径下打开cmd（在文件夹下的路径输入框中输入cmd并回车），或者直接在cmd中切换到该文件夹下，输入：1hexo init 运行后的样子，其中在下载默认主题landscape时，出现了一些乱码，不过这些问题不大，最后显示添加add成功就行：此时文件夹下多了如下一些内容： 本地运行此时还是在该文件夹路径下的cmd里输入（-p 5000 表示设置端口号为5000；若不写这个，则默认为4000，但可能会出现端口号被占用的情况，导致网页无法打开）：1hexo server -p 5000 此时浏览器上输入http://localhost:5000/，访问到博客的默认界面： hexo相关命令新建某一博客，文件名为this_is_a_test_blog.md，此为markdown文件，文件路径为source\_posts\1hexo new this_is_a_test_blog 我们可以在这this_is_a_test_blog.md文件里添加自己的博客内容，因为我是用作测试的，所以什么内容这边就不贴图了。重新运行hexo server -p 5000，访问localhost可以看到，网页中有了内容添加。 其他相关的命令如下：hexo clean 删除public文件夹及其内容（public文件夹的内容即为上传至GitHub Repositoriy的内容）hexo generate或hexo g 生成上传至GitHub的内容，即public文件夹hexo deploy或hexo d 上传至GitHub（需进行配置） 这时候，博客的基本设置（主题设置留待以后）都已弄好，接下来就是解决将博客内容上传至GitHub上的问题。 Git下载安装传送门：git，选择对应版本下载安装。安装过程这边也不叙述了，就点点点。安装完成后，看下版本git --version 配置个人信息打开Git Bash，配置个人信息，分别输入如下命令，yourname即GitHub注册时的用户名，yourEmail即注册时的邮箱账号，以及生成秘钥：1234git config --global user.name &quot;yourname&quot;git config --global user.email &quot;yourEmail&quot;ssh-keygen -t rsa -C &quot;yourEmail&quot; 在秘钥生成后，会有对应的存放文件地址，去该地址中，找到id_rsa.pub文件，复制里面的内容，粘贴至GitHub中，点击右上角用户头像下的Settings，之后点击左侧的SSH and GPG keys，找到New SSH key点击，输入title，并将之前复制的内容粘贴到下面的key中，最后点击Add SSH key，完成： 配置Deployment去博客总目录下的_config.yml文件中，找到deploy部分，添加如下（yourname就是GitHub的用户名）：1234deploy: type: git repo: https://github.com/yourname/yourname.github.io.git branch: master 整体流程至此，除了博客主题（目前采用默认主题）外，其他基本设置都已弄好。整体的写文上传等操作流程为：hexo new newBloghexo cleanhexo ghexo d]]></content>
      <categories>
        <category>GitBlog</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Git</tag>
        <tag>Hexo</tag>
        <tag>Node.js</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中秋节快乐！]]></title>
    <url>%2F2017%2F10%2F04%2Fmidautumn%2F</url>
    <content type="text"><![CDATA[祝所有走过路过的亲们 中秋节快乐，幸福如意！时间：2017-10-04节日：中秋佳节地点：笔者家]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法学习之蛮力法]]></title>
    <url>%2F2017%2F09%2F21%2Fbruteforce%2F</url>
    <content type="text"><![CDATA[一种简单直接地解决问题的方法，常常直接基于问题的描述和所涉及的概念定义。 算法设计策略 – 蛮力法选择排序和冒泡排序选择排序基本原理 扫描整个列表，找到最小元素，将其和第一个元素进行交换，此时最小元素在有序表中的最终位置上； 从第二元素开始扫描列表，找到最后n-1个元素中的最小元素，再和第二个元素交换位置； 之后，在对该列表做第i遍扫描时(i值为0~n-2)，在最后n-i个元素中寻找最小元素，然后将之和 Ai 交换； 在经过n-1遍后，列表已序。 示例：1234567| 89 45 68 90 29 34 17 17 | 45 68 90 29 34 89 17 29 | 68 90 45 34 89 17 29 34 | 90 45 68 89 17 29 34 45 | 90 68 89 17 29 34 45 68 | 90 89 17 29 34 45 68 89 | 90 代码实现伪代码： 12345678910算法 SelectionSort(A[0..n-1]) //该算法用选择排序对给定列表排序 //输入: 一个可排序数组 A[0..n-1] //输出: 升序排列的数组 A[0..n-1] for i = 0 to n - 2 do min = i for j = i + 1 to n - 1 do if A[j] &lt; A[min] min = j swap A[i] and A[min] python实现： 1234567891011def SelectionSort(A): #选择排序 对给定列表排序(升序) #输入: 待排序列表 A[0..n-1] list #输出: 已排序列表 A[0..n-1] list lenA = len(A) for i in range(lenA-1): minValIndex = i for j in range(i+1, lenA): if A[minValIndex] &gt; A[j]: minValIndex = j A[i], A[minValIndex] = A[minValIndex], A[i] 冒泡排序基本原理 从第一个元素开始，依次比较相邻元素值，即第一个元素和第二个元素比较，若逆序，则交换位置； 第二个元素和第三个元素比较，若逆序，交换位置； 依次下去，则将最大元素移动到了最右端（“沉到最底部”）； 再次从列表头开始依次比较相邻数，重复多次后，第二大的数已移到最右第二个位置上； 多次比较后，列表已序。 示例：1234567891011121314151689 &lt;?&gt; 45 68 90 29 34 1745 89 &lt;?&gt; 68 90 29 34 1745 68 89 &lt;?&gt; 90 29 34 1745 68 89 90 &lt;?&gt; 29 34 1745 68 89 29 90 &lt;?&gt; 34 1745 68 89 29 34 90 &lt;?&gt; 1745 68 89 29 34 17 |9045 &lt;?&gt; 68 89 29 34 17 |9045 68 &lt;?&gt; 89 29 34 17 |9045 68 89 &lt;?&gt; 29 34 17 |9045 68 29 89 &lt;?&gt; 34 17 |9045 68 29 34 89 &lt;?&gt; 17 |9045 68 29 34 17 |89 90etc. 代码实现伪代码：12345678算法 BubbleSort(A[0..n-1]) //该算法用冒泡排序对数组A[0..n-1]进行排序 //输入：一个可排序数组A[0..n-1] //输出：非降序排列的数组A[0..n-1] for i = 0 to n - 2 do for j = 0 to n - 2 - i do if A[j+1] &lt; A[j] swap A[j] and A[j+1] python实现123456789def BubbleSort(A): #冒泡排序 对给定列表排序(升序) #输入: 待排序列表 A[0..n-1] list #输出: 已排序列表 A[0..n-1] list lenA = len(A) for i in range(lenA-1): for j in range(lenA-1-i): if A[j+1] &lt; A[j]: A[j], A[j+1] = A[j+1], A[j]]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Brute Force</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博文]]></title>
    <url>%2F2017%2F09%2F16%2Ffirstblog%2F</url>
    <content type="text"><![CDATA[因为感觉自己若能有一个独立的博客网站，能够在上面写写东西，能够把自己学到的东西给记录下来，这绝对是一件很cool的事，所以就有了这个博客网站。因此自己特地花上两三天时间，终于把一个能够搬上台面的博客网站给整出来了。当然，目前网站里也只是完成了一些基本功能的整合，还有许多常用的功能尚未组装进来，所以后续的会继续进行完善。 写这个博文，第一是想庆祝下，自己这几天的忙绿终有结果，看着还不错 (￣▽￣)~*第二就是想提醒下自己，在以后的学习成长过程中最好能够把所看到的、所学到的内容整理出来，方便理清思路及以后的回顾，搭建这个博客的初衷也就是这；最后，希望自己能够稳步前行。 over！]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>Life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
